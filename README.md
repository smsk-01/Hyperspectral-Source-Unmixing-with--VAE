# üõ∞Ô∏è Beta-VAE for Hyperspectral Unmixing

A **deep learning framework** for hyperspectral unmixing using **Variational Autoencoders** with **Dirichlet latent distributions**, achieving **state-of-the-art performance** on benchmark datasets.

---

## üìò Overview

Hyperspectral unmixing decomposes each pixel spectrum into **pure material spectra (endmembers)** and their **fractional abundances**.  
This work introduces a **Beta-VAE** architecture leveraging the **Dirichlet distribution**‚Äôs natural **simplex constraint** to model abundance maps **probabilistically and physically consistently**.

---

## üåà Hyperspectral Unmixing Problem

### Linear Mixing Model

We assume a **linear mixture** of $M$ endmembers across $N$ pixels and $L$ spectral bands:

$$
\mathbf{Y} = \mathbf{E}\mathbf{A} + \mathbf{N}
$$

where:

- $\mathbf{Y} \in \mathbb{R}^{L\times N}$ ‚Äî observed hyperspectral data  
- $\mathbf{E} \in \mathbb{R}^{L\times M}$ ‚Äî endmember matrix  
- $\mathbf{A} \in \mathbb{R}^{M\times N}$ ‚Äî abundance matrix  
- $\mathbf{N} \in \mathbb{R}^{L\times N}$ ‚Äî additive noise  

### Constraints

Each pixel‚Äôs abundances must satisfy:

$$
a_{m,n} \ge 0 \quad \forall m,n
$$

$$
\sum_{m=1}^{M} a_{m,n} = 1 \quad \forall n
$$

so that $\mathbf{A}$ lies on a **unit simplex**.

---

##  State-of-the-Art Methods
<p align="center">
  <img src="endmembers.png" alt="Model Architecture" width="55%">
  <img src="abundances.png" alt="Reconstruction Results" width="55%">
</p>


| Type | Example | Limitation |
|------|----------|-------------|
| Geometric | N-FINDR, VCA | Sensitive to noise |
| Analytical | FCLS | Requires known endmembers |
| Deep learning | CNN-based AE/UNet | Weak simplex enforcement |

**Most existing approaches** fail to jointly:
- enforce **sum-to-one** naturally,  
- estimate **endmembers and abundances together**,  
- handle **spectral variability** robustly.

---

##  Proposed Method: Dirichlet-VAE

###  Key Idea ‚Äî Dirichlet Latent Space

We adopt a **Dirichlet latent prior**:

$$
\text{Dir}(\boldsymbol{\alpha}) = \frac{\Gamma\!\left(\sum_i \alpha_i\right)}{\prod_i \Gamma(\alpha_i)} \prod_i x_i^{\alpha_i - 1}
$$

where $\boldsymbol{\alpha} = [\alpha_1, \dots, \alpha_M]$ are concentration parameters.  
This prior ensures that sampled abundances **naturally satisfy** the simplex constraints.

---

###  Architecture

**Encoder**

$$
\mathbf{h} = \text{Encoder}(\mathbf{x}), \qquad
\boldsymbol{\alpha} = \text{Softplus}(\text{MLP}(\mathbf{h})) + 1
$$

**Latent Sampling**

$$
\mathbf{a} \sim \text{Dir}(\boldsymbol{\alpha}) \quad \text{(training)}, \qquad
\mathbf{a} = \frac{\boldsymbol{\alpha}}{\sum_i \alpha_i} \quad \text{(inference)}
$$

**Decoder**

$$
\hat{\mathbf{x}} = \mathbf{E}^\top \mathbf{a}
$$

where $\mathbf{E}$ is a learnable endmember matrix.

---

##  Loss Function

The overall loss combines **spectral reconstruction** and **Dirichlet regularization**:

### 1Ô∏è‚É£ Reconstruction Loss

$$
\mathcal{L}_{\text{recon}} = \text{SAD}(\mathbf{x}, \hat{\mathbf{x}}) + 0.1\,\text{MSE}(\mathbf{x}, \hat{\mathbf{x}})
$$

where

$$
\text{SAD}(\mathbf{x}, \hat{\mathbf{x}}) = 
\frac{\arccos\!\left(\frac{\langle \mathbf{x}, \hat{\mathbf{x}} \rangle}{\|\mathbf{x}\|_2 \|\hat{\mathbf{x}}\|_2}\right)}{\pi}
$$

### 2Ô∏è‚É£ KL Divergence

Regularization towards a **uniform Dirichlet** ($\text{Dir}(\mathbf{1})$):

$$
\mathcal{L}_{\text{KL}} =
\log \Gamma\!\left(\sum_i \alpha_i\right)
- \sum_i \log \Gamma(\alpha_i)
- \log \Gamma(M)
+ \sum_i (\alpha_i - 1)\left[\psi(\alpha_i) - \psi\!\left(\sum_j \alpha_j\right)\right]
$$

### 3Ô∏è‚É£ Total Objective

$$
\mathcal{L} = \lambda_{\text{recon}} \, \mathcal{L}_{\text{recon}} + \lambda_{\text{KL}} \, \mathcal{L}_{\text{KL}}
$$

---

## üßÆ Training Details

- Optimizer: **AdamW** (weight decay)
- Regularization: **KL annealing**
- Stability: **Gradient clipping**
- Scheduling: **ReduceLROnPlateau**
- Early stopping for convergence

---

## üìä Evaluation Metrics

| Metric | Purpose |
|---------|----------|
| **SAM (¬∞)** | Endmember similarity |
| **RMSE / MAE** | Abundance accuracy |
| **SAD** | Spectral reconstruction quality |

---

## üìà Results

| Dataset | Method | Endmember RMSE ‚Üì | Abundance RMSE ‚Üì |
|----------|---------|------------------|------------------|
| **Samson** | N-FINDR + FCLS | 0.0847 | 0.1234 |
| **Samson** | CNN-based | 0.0621 | 0.0987 |
| **Samson** | **Dirichlet-VAE (ours)** | **0.0423** | **0.0756** |

‚úÖ **+32% improvement** in endmember estimation  
‚úÖ **+23% improvement** in abundance estimation  
‚úÖ **No post-processing normalization needed**

---

## üíª Usage

```python
from model import VAE
from trainer import train_vae
import torch

vae = VAE(nb_channels=156, n_sources=3)
train_losses = train_vae(vae, train_loader, epochs=100)

vae.eval()
with torch.no_grad():
    output, alpha, abundances = vae(hyperspectral_data)
    endmembers = vae.endmembers
